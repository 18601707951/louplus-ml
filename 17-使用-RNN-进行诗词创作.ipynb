{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='brown'>楼 + 机器学习实战</font>\n",
    "\n",
    "# 挑战：使用 RNN 完成诗词创作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑战介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一个实验中，我们学习了循环神经网络 RNN 的相关知识，也搭建了循环神经网络完成了情感分析的任务。同时，实验进一步了解了经典循环神经网络结构。本次挑战中，我们需要把学到的知识运用起来，通过利用循环神经网络这个强大的工具，完成诗词创作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑战知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras 运用\n",
    "- 循环神经网络实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #999;font-size: 12px;\">* 本挑战项目参考 [ioiogoo/poetry_generator_Keras](https://github.com/ioiogoo/poetry_generator_Keras)，并对部分内容进行改进。</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑战内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环神经网络在自然语言处理有着极为广泛的运用。像文本，语音这样的序列模型，使用循环神经网络处理再顺心不过了。今天的挑战中，将利用从网络爬取到的古诗词做为训练集，让循环神经网络学习这些诗词后，根据提示写出诗句。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，下载本次挑战所使用到的文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://labfile.oss.aliyuncs.com/courses/1081/rnn_poetry.zip # 下载数据集\n",
    "!unzip rnn_poetry.zip # 解压数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑战使用到的数据集来源于网络，包含有 2 万余首古诗词。我们随意截取其中一首如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; color:brown;\">\n",
    "襄阳行乐处，歌舞白铜鞮。<br>江城回渌水，花月使人迷。<br>山公醉酒时，酩酊襄阳下。<br>头上白接篱，倒着还骑马。<br>岘山临汉江，水渌沙如雪。<br>上有堕泪碑，青苔久磨灭。<br>且醉习家池，莫看堕泪碑。<br>山公欲上马，笑杀襄阳儿。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，数据集中都是十分有意境的古诗词。对于文本数据，代码是无法像我们人一样直接看懂。所以，这就需要一些向量化、数值化的手段将其处理成机器可以识别的输入。下面，我们通过挑战提供的 `preprocess_file()` 方法对预料进行预处理，这里只保留五言绝句的预料以保证后面输出稳定。\n",
    "\n",
    "经过文本预处理之后，依次返回：字到序号映射字典、序号到字映射字典、数据集中全部单字、数据集全文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_clean import preprocess_file\n",
    "\n",
    "# 依次返回：字到序号映射字典、序号到字映射字典、数据集中全部单字、数据集全文\n",
    "word2num, num2word, words, files_content = preprocess_file(\"poetry.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把单个字处理成**序号和字**的对应关系，每一首诗词就可以由文字表示变为数字表示，这样才能作为输入被用于训练 RNN 网络。你可以自行输出字典键值查看，以便于更加熟悉数据集，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2word[10], num2word[20], num2word[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立 LSTM 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们构建 LSTM 结构的循环神经网络模型。模型结构如下：\n",
    "\n",
    "- LSTM 层：输出 `512`，输入形状为 `(6, len(words))`，设置 `return_sequences=True`。\n",
    "- Dropout 层：概率为 `0.6`。\n",
    "- LSTM 层：输出 `256`。\n",
    "- Dropout 层：概率为 `0.6`。\n",
    "- 全连接层：输出为 `len(words)`，使用 `softmax` 激活。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用多分类交叉熵损失函数 `categorical_crossentropy`，同时使用 Adam 优化器，学习率为 `0.001`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>挑战</font>：根据上述规定，使用 Keras 序贯模型搭建方法构建 LSTM 模型。**（未提到参数使用默认值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "\"\"\"定义序贯模型结构\n",
    "\"\"\"\n",
    "\n",
    "### 代码开始 ### (≈ 6 行代码)\n",
    "model = Sequential()\n",
    "model.add(layers.LSTM(512, return_sequences=True, input_shape=(6, len(words))))\n",
    "model.add(layers.Dropout(0.6))\n",
    "model.add(layers.LSTM(256))\n",
    "model.add(layers.Dropout(0.6))\n",
    "model.add(layers.Dense(len(words), activation='softmax'))\n",
    "### 代码结束 ###\n",
    "\n",
    "\"\"\"优化器和损失函数\n",
    "\"\"\"\n",
    "\n",
    "### 代码开始 ### (≈ 2 行代码)\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "### 代码结束 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**运行测试：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**期望输出：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    lstm_1 (LSTM)                (None, 6, 512)            12421120  \n",
    "    _________________________________________________________________\n",
    "    dropout_1 (Dropout)          (None, 6, 512)            0         \n",
    "    _________________________________________________________________\n",
    "    lstm_2 (LSTM)                (None, 256)               787456    \n",
    "    _________________________________________________________________\n",
    "    dropout_2 (Dropout)          (None, 256)               0         \n",
    "    _________________________________________________________________\n",
    "    dense_1 (Dense)              (None, 5552)              1426864   \n",
    "    =================================================================\n",
    "    Total params: 14,635,440\n",
    "    Trainable params: 14,635,440\n",
    "    Non-trainable params: 0\n",
    "    _________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的模型结构非常简单，也就是两个 LSTM 层的堆叠。但是，需要训练的参数非常多，已经超过千万。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，就是训练模型的过程。在这部分中，我们会定义多个函数以便达到目的。如果你有兴趣，可以了解每个函数的用途。当然，如果想更快看到结果，直接点击运行就可以了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "\n",
    "max_len = 6  # 五言绝句包含标点\n",
    "poems = files_content.split(']')  # 诗的 list\n",
    "poems_num = len(poems)  # 诗的总数量\n",
    "\n",
    "def word2numF(x): return word2num.get(x, len(words) - 1)\n",
    "\n",
    "def sample(preds):\n",
    "    exp_preds = np.asarray(preds).astype('float64')\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    pro = np.random.choice(range(len(preds)), 1, p=preds)\n",
    "    return int(pro.squeeze())\n",
    "\n",
    "# 根据给出的首个文字，生成五言绝句\n",
    "def predict_first(char):\n",
    "    index = np.random.randint(0, poems_num)\n",
    "    sentence = poems[index][1-max_len:] + char\n",
    "    generate = str(char)\n",
    "    generate += _preds(sentence, length=23)\n",
    "    return generate\n",
    "\n",
    "# 内部方法，输入 max_len 长度字符串，返回 length 长度的预测值字符串\n",
    "def _preds(sentence, length=23):\n",
    "    sentence = sentence[:max_len]\n",
    "    generate = ''\n",
    "    for i in range(length):\n",
    "        pred = _pred(sentence)\n",
    "        generate += pred\n",
    "        sentence = sentence[1:]+pred\n",
    "    return generate\n",
    "\n",
    "# 内部方法，根据一串输入，返回单个预测字符\n",
    "def _pred(sentence):\n",
    "    sentence = sentence[-max_len:]\n",
    "    x_pred = np.zeros((1, max_len, len(words)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, word2numF(char)] = 1.\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = num2word[next_index]\n",
    "\n",
    "    return next_char\n",
    "\n",
    "# 生成数据\n",
    "def data_generator():\n",
    "    i = 0\n",
    "    while 1:\n",
    "        x = files_content[i: i + max_len]\n",
    "        y = files_content[i + max_len]\n",
    "\n",
    "        if ']' in x or ']' in y:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        y_vec = np.zeros(\n",
    "            shape=(1, len(words)),\n",
    "            dtype=np.bool\n",
    "        )\n",
    "        y_vec[0, word2numF(y)] = 1.0\n",
    "\n",
    "        x_vec = np.zeros(\n",
    "            shape=(1, max_len, len(words)),\n",
    "            dtype=np.bool\n",
    "        )\n",
    "\n",
    "        for t, char in enumerate(x):\n",
    "            x_vec[0, t, word2numF(char)] = 1.0\n",
    "\n",
    "        yield x_vec, y_vec\n",
    "        i += 1\n",
    "\n",
    "# 训练过程中，每 5 个 epoch 打印出当前的学习情况\n",
    "def generate_sample_result(epoch, logs):\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # 随机获取某首诗词中的首字用于测试  \n",
    "        char = poems[np.random.randint(len(poems))][0]\n",
    "        generate = predict_first(char)\n",
    "        print(\"+++++++++++++ 随机测试: {} +++++++++++++\".format(char))\n",
    "        print(generate)\n",
    "    \n",
    "# 训练模型\n",
    "def train(epochs):\n",
    "    model.fit_generator(\n",
    "        generator=data_generator(),\n",
    "        verbose=True,\n",
    "        steps_per_epoch=32,\n",
    "        epochs=epochs,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint('poetry_model.h5'),\n",
    "            LambdaCallback(on_epoch_end=generate_sample_result)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们设定训练 2000 个 epoch。每一个 epoch 之后模型都会被保存，然后我们从字典中随机挑选出一个字符作为诗的第一个字，用于测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(epochs=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会发现，最开始写出来的五言绝句没有正确的格式、且无法正确使用标点。但是，随着迭代次数的增加，测试结果变得越来越好。最后，我们给出 3000 个 epoch 之后，机器以「风」开头创作的 3 首诗："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; color:brown\">\n",
    "风北溪爱风，山树来多白。衣过星殿望，野方林为野。<br>\n",
    "风思归明月，荷鸟接今方。翼年游得登，雪明将在人。<br>\n",
    "风南盖酒羽，溪无可临楼。北未晚散野，临可水幽羽。<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #999;font-size: 12px;font-style: italic;\">*本课程内容，由作者授权实验楼发布，未经允许，禁止转载、下载及非法传播。</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
